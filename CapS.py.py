# -*- coding: utf-8 -*-
"""Untitled43.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ejiNnUyGN0Rwpr4O3Bv0cM_xOLYXHzHS

# Capstone Project - To find a good location for New Coffee Shop 
### Applied Data Science Capstone by IBM/Coursera

## Table of contents
* [Introduction: Business Problem](#introduction)
* [Data](#data)
* [Methodology](#methodology)
* [Analysis](#analysis)
* [Results and Discussion](#results)
* [Conclusion](#conclusion)

## Introduction: Business Problem <a name="introduction"></a>

This project meant to find an optimal location for a coffee shop. Specifically, this report will be targeted to stakeholders interested in opening a coffee shop in **Shanghai, China**, my home town BTW.

Since there are lots of coffee shops in SH I will try to detect locations that are not already crowded with coffee shops. **Location matters**.So,the most important thing for those who want to drive a successful cafe shop is if it has been set on a good area that without much competition.

I'll use our data science powers to **cluster the coffee shops** arround and to figue out the traits they have. Then the results might be intersted by the stakeholders.

## Data <a name="data"></a>

Based on definition of our problem, factors that will influence our decission are:
* number of existing coffee shop in the neighborhood
* distance of neighborhood from city center

I decided to use regularly spaced grid of locations, centered around city center, to define our neighborhoods , according to others suggusting,btw.

Following data sources will be needed to extract/generate the required information:
* centers of candidate areas will be generated algorithmically and approximate addresses of centers of those areas will be obtained using **Google Maps API reverse geocoding**
* number of coffee shops and their type and location in every neighborhood will be obtained using **Foursquare API**
* coordinate of ShangHai center will be obtained using **Google Maps API geocoding** of well known ShangHai location (People square)

### Neighborhood Candidates

Let's create latitude & longitude coordinates for centroids of our candidate neighborhoods. We will create a grid of cells covering our area of interest which is aprox. 12x12 killometers centered around SH city center.

Let's first find the latitude & longitude of SH city center, using specific, well known address and Google Maps geocoding API.
"""

##log_in to API here...

import requests


def get_coordinates(api_key, address, verbose=False):
    try:
        url = 'https://maps.googleapis.com/maps/api/geocode/json?key={}&address={}'.format(api_key, address)
        response = requests.get(url).json()
        if verbose:
            print('Google Maps API JSON result =>', response)
        results = response['results']
        geographical_data = results[0]['geometry']['location'] # get geographical coordinates
        lat = geographical_data['lat']
        lon = geographical_data['lng']
        return [lat, lon]
    except:
        return [None, None]
    
address = 'People Square, ShangHai, China'
SH_center = get_coordinates(google_api_key, address)
print('Coordinate of {}: {}'.format(address, SH_center))

"""Now let's create a grid of area candidates, equaly spaced, centered around city center and within ~16km from People Square, center of SH. Our neighborhoods will be defined as circular areas with a radius of 500 meters, so our neighborhood centers will be 1000 meters apart.

To accurately calculate distances we need to create our grid of locations in Cartesian 2D coordinate system which allows us to calculate distances in meters (not in latitude/longitude degrees). Then we'll project those coordinates back to latitude/longitude degrees to be shown on Folium map. So let's create functions to convert between WGS84 spherical coordinate system (latitude/longitude degrees) and UTM Cartesian coordinate system (X/Y coordinates in  meters).
"""

#!pip install shapely
import shapely.geometry

!pip install pyproj
import pyproj

import math

def lonlat_to_xy(lon, lat):
    proj_latlon = pyproj.Proj(proj='latlong',datum='WGS84')
    proj_xy = pyproj.Proj(proj="utm", zone=33, datum='WGS84')
    xy = pyproj.transform(proj_latlon, proj_xy, lon, lat)
    return xy[0], xy[1]

def xy_to_lonlat(x, y):
    proj_latlon = pyproj.Proj(proj='latlong',datum='WGS84')
    proj_xy = pyproj.Proj(proj="utm", zone=33, datum='WGS84')
    lonlat = pyproj.transform(proj_xy, proj_latlon, x, y)
    return lonlat[0], lonlat[1]

def calc_xy_distance(x1, y1, x2, y2):
    dx = x2 - x1
    dy = y2 - y1
    return math.sqrt(dx*dx + dy*dy)

print('Coordinate transformation check')
print('-------------------------------')
print('ShangHai center longitude={}, latitude={}'.format(SH_center[1], SH_center[0]))
x, y = lonlat_to_xy(SH_center[1], SH_center[0])
print('ShangHai center UTM X={}, Y={}'.format(x, y))
lo, la = xy_to_lonlat(x, y)
print('ShangHai center longitude={}, latitude={}'.format(lo, la))

"""Let's create a hexagonal grid of cells: we offset every other row, and adjust vertical row spacing so that every cell center is equally distant from all it's neighbors."""

SH_center_x, SH_center_y = lonlat_to_xy(SH_center[1], SH_center[0]) # City center in Cartesian coordinates

k = math.sqrt(3)/2  # Vertical offset for hexagonal grid cells
x_min = SH_center_x - 16000
x_step = 1600
y_min = SH_center_y - 16000 - (int(21/k)*k*1600 - 32000)/2

y_step = 1600 * k 

latitudes = []
longitudes = []
distances_from_center = []
xs = []
ys = []
for i in range(0, int(21/k)):
    y = y_min + i * y_step
    x_offset = 800 if i%2==0 else 0
    for j in range(0, 21):
        x = x_min + j * x_step + x_offset
        distance_from_center = calc_xy_distance(SH_center_x, SH_center_y, x, y)
        if (distance_from_center <= 16001):
            lon, lat = xy_to_lonlat(x, y)
            latitudes.append(lat)
            longitudes.append(lon)
            distances_from_center.append(distance_from_center)
            xs.append(x)
            ys.append(y)

print(len(latitudes), 'candidate neighborhood centers generated.')

"""Let's visualize the data we have so far: city center location and candidate neighborhood centers:"""

import folium

map_SH = folium.Map(location=SH_center, zoom_start=13)
folium.Marker(SH_center, popup='PeopleSquare').add_to(map_SH)
for lat, lon in zip(latitudes, longitudes):
    #folium.CircleMarker([lat, lon], radius=2, color='blue', fill=True, fill_color='blue', fill_opacity=1).add_to(map_SH) 
    folium.Circle([lat, lon], radius=500, color='blue', fill=False).add_to(map_SH)
    #folium.Marker([lat, lon]).add_to(map_SH)
map_SH



"""OK, we now have the coordinates of centers of neighborhoods/areas to be evaluated, equally spaced (distance from every point to it's neighbors is exactly the same) and within ~16km from People Squre. 

Let's now use Google Maps API to get approximate addresses of those locations.
"""

def get_address(api_key, latitude, longitude, verbose=False):
    try:
        url = 'https://maps.googleapis.com/maps/api/geocode/json?key={}&latlng={},{}'.format(api_key, latitude, longitude)
        response = requests.get(url).json()
        if verbose:
            print('Google Maps API JSON result =>', response)
        results = response['results']
        address = results[0]['formatted_address']
        return address
    except:
        return None

addr = get_address(google_api_key, SH_center[0], SH_center[1])
print('Reverse geocoding check')
print('-----------------------')
print('Address of [{}, {}] is: {}'.format(SH_center[0], SH_center[1], addr))

print('Obtaining location addresses: ', end='')
addresses = []
for lat, lon in zip(latitudes, longitudes):
    address = get_address(google_api_key, lat, lon)
    if address is None:
        address = 'NO ADDRESS'
    address = address.replace(', China', '') # replace work for str only
    #re...
    addresses.append(address)
    print(' .', end='')
print(' done.')

addresses[150:170]



"""See.Let's now place all this into a Pandas dataframe."""

import pandas as pd

df_locations = pd.DataFrame({'Address': addresses,
                             'Latitude': latitudes,
                             'Longitude': longitudes,
                             'X': xs,
                             'Y': ys,
                             'Distance from center': distances_from_center})

df_locations.head(10)

len(df_locations.Address)

"""...and let's now save/persist this data into local file."""

df_locations.to_pickle('./locations.pkl')

#with open('/content/locations (1).pkl', 'rb') as f:
#   df_locations = pickle.load(f)

"""### Foursquare
Now that we have our location candidates, let's use Foursquare API to get info on coffeeshops in each neighborhood.

We're interested in venues in 'coffee shop' category.
"""

df_locations.head()

"""Foursquare credentials are defined in hidden cell bellow."""

# Category IDs corresponding to Italian coffeeshops were taken from Foursquare web site (https://developer.foursquare.com/docs/resources/categories):

coffee_shop_category = '4bf58dd8d48988d1e0931735' # 'Root' category for all food-related venues


def is_coffeeshop(categories):
    coffeeshop_words = ['coffeeshop', 'coffeehouse', 'coffee']
    coffeeshop = False
    for c in categories:
        category_name = c[0].lower()
        category_id = c[1]
        for r in coffeeshop_words:
            if r in category_name:
                coffeeshop = True
    return coffeeshop

def get_categories(categories):
    return [(cat['name'], cat['id']) for cat in categories]

def format_address(location):
    address = ', '.join(location['formattedAddress'])
    address = address.replace(', Chinese', '')
    address = address.replace(', China', '')
    return address

def get_venues_near_location(lat, lon, category, client_id, client_secret, radius=500, limit=100):
    version = '20180724'
    url = 'https://api.foursquare.com/v2/venues/explore?client_id={}&client_secret={}&v={}&ll={},{}&categoryId={}&radius={}&limit={}'.format(
        client_id, client_secret, version, lat, lon, category, radius, limit)
    try:
        results = requests.get(url).json()['response']['groups'][0]['items']
        venues = [(item['venue']['id'],
                   item['venue']['name'],
                   get_categories(item['venue']['categories']),
                   (item['venue']['location']['lat'], item['venue']['location']['lng']),
                   format_address(item['venue']['location']),
                   item['venue']['location']['distance']) for item in results]        
    except:
        venues = []
    return venues

# Let's now go over our neighborhood locations and get nearby coffeeshops; we'll also maintain a dictionary of all found coffeeshops and all found italian coffeeshops

import pickle

def get_coffeeshops(lats, lons):
    coffeeshops = {}
    location_coffeeshops = []

    print('Obtaining venues around candidate locations:', end='')
    for lat, lon in zip(lats, lons):
        # Using radius=500 to meke sure we have overlaps/full coverage so we don't miss any coffeeshop (we're using dictionaries to remove any duplicates resulting from area overlaps)
        venues = get_venues_near_location(lat, lon, coffee_shop_category, CLIENT_ID, CLIENT_SECRET, radius=500, limit=100)
        area_coffeeshops = []
        for venue in venues:
            venue_id = venue[0]
            venue_name = venue[1]
            venue_categories = venue[2]
            venue_latlon = venue[3]
            venue_address = venue[4]
            venue_distance = venue[5]
            is_cof = is_coffeeshop(venue_categories)
            if is_cof:
                x, y = lonlat_to_xy(venue_latlon[1], venue_latlon[0])
                coffeeshop = (venue_id, venue_name, venue_latlon[0], venue_latlon[1], venue_address, venue_distance, x, y)
                if venue_distance<=500:
                    area_coffeeshops.append(coffeeshop)
                coffeeshops[venue_id] = coffeeshop

        location_coffeeshops.append(area_coffeeshops)
        print(' .', end='')
    print(' done.')
    return coffeeshops, location_coffeeshops

# Try to load from local file system in case we did this before
coffeeshops = {}
location_coffeeshops = []
loaded = False
try:
    with open('coffeeshops_500.pkl', 'rb') as f:
        coffeeshops = pickle.load(f)
    with open('location_coffeeshops_500.pkl', 'rb') as f:
        location_coffeeshops = pickle.load(f)
    print('coffeeshop data loaded.')
    loaded = True
except:
    pass

# If load failed use the Foursquare API to get the data
if not loaded:
    coffeeshops, location_coffeeshops = get_coffeeshops(latitudes, longitudes)
    
    # Let's persists this in local file system
    with open('coffeeshops_500.pkl', 'wb') as f:
        pickle.dump(coffeeshops, f)
    with open('location_coffeeshops_500.pkl', 'wb') as f:
        pickle.dump(location_coffeeshops, f)

len(coffeeshops)

import numpy as np

print('Total number of coffeeshops:', len(coffeeshops))
print('Average number of coffeeshops in neighborhood:', np.array([len(r) for r in location_coffeeshops]).mean())

print('List of all coffeeshops')
print('-----------------------')
for r in list(coffeeshops.values())[:10]:
    print(r)
print('...')
print('Total:', len(coffeeshops))

print('coffeeshops around location')
print('---------------------------')
for i in range(100, 110):
    rs = location_coffeeshops[i][:8]
    names = ', '.join([r[1] for r in rs])
    print('coffeeshops around location {}: {}'.format(i+1, names))

"""Let's now see all the collected coffeeshops in our area of interest on map."""

map_SH = folium.Map(location=SH_center, zoom_start=13)
folium.Marker(SH_center, popup='People Square').add_to(map_SH)
for res in coffeeshops.values():
    lat = res[2]; lon = res[3]
    color = 'red' 
    folium.CircleMarker([lat, lon], radius=3, color=color, fill=True, fill_color=color, fill_opacity=1).add_to(map_SH)
map_SH

coffeeshops

"""That's a lot...
So, if i were a guy who want to start a coffee shop, I will avoid crowed location and try to approach the Center of city.

## Methodology <a name="methodology"></a>

In this project we will direct our efforts on detecting areas of SH that have low coffeeshop density, particularly those with low number of Italian coffeeshops. We will limit our analysis to area ~16km around city center.

In first step we have collected the required **data: location and type (category) of every coffeeshop within 16km from SH center** .

Second step in our analysis will be calculation and exploration of '**coffeeshop density**' across different areas of SH - we will use **heatmaps** to identify a few promising areas close to center with low number of coffeeshops in general (*and* no Italian coffeeshops in vicinity) and focus our attention on those areas.

In third and final step we will focus on most promising areas and within those create **clusters of locations that meet some basic requirements** established in discussion with stakeholders.

## Analysis <a name="analysis"></a>

Let's perform some basic explanatory data analysis and derive some additional info from our raw data. First let's count the **number of coffeeshops in every area candidate**:
"""

location_coffeeshops_count = [len(res) for res in location_coffeeshops]

df_locations['coffeeshops in area'] = location_coffeeshops_count

print('Average number of coffeeshops in every area with radius=500m:', np.array(location_coffeeshops_count).mean())

df_locations.head(10)



"""Let's crete a map showing **heatmap / density of coffeeshops** and try to extract some meaningfull info from that. Also, let's show **borders of SH boroughs** on our map and a few circles indicating distance of 1km, 2km and 3km from Alexanderplatz."""

coffeeshop_latlons = [[res[2], res[3]] for res in coffeeshops.values()]

from folium import plugins
from folium.plugins import HeatMap

map_SH = folium.Map(location=SH_center, zoom_start=13)
folium.TileLayer('cartodbpositron').add_to(map_SH) #cartodbpositron cartodbdark_matter
HeatMap(coffeeshop_latlons).add_to(map_SH)
folium.Marker(SH_center).add_to(map_SH)
folium.Circle(SH_center, radius=3000, fill=False, color='white').add_to(map_SH)
folium.Circle(SH_center, radius=6000, fill=False, color='white').add_to(map_SH)
folium.Circle(SH_center, radius=9000, fill=False, color='white').add_to(map_SH)
#folium.GeoJson(SH_boroughs, style_function=boroughs_style, name='geojson').add_to(map_SH)
map_SH

df_locations.head()

"""###Let's cluster them to find out if there is a good location for us to erect a coffee shop."""

from sklearn.cluster import KMeans

number_of_clusters = 36

crowedareas = df_locations[['X', 'Y']].values
kmeans = KMeans(init='k-means++',n_clusters=number_of_clusters, random_state=0).fit(crowedareas)

cluster_centers = [xy_to_lonlat(cc[0], cc[1]) for cc in kmeans.cluster_centers_]

labels = kmeans.labels_

df_locations['labels'] = labels;df_locations

df_locations.to_pickle('./df_locations.pkl')

"""#Results and Discussion...

Our analysis shows that although there is a great number of coffee shop in ShangHai, there are pockets of low coffee shop density fairly close to city center. Highest concentration of coffee shops was detected north and west from Alexanderplatz, so we focused our attention to areas north-east and east. 

Those location candidates were then clustered to create zones of interest which contain greatest number of location candidates. Addresses of centers of those zones were also generated using reverse geocoding to be used as markers/starting points for more detailed local analysis based on other factors.

Result of all this is 15 zones containing largest number of potential new coffee shop locations based on number of and distance to existing venues - both coffee shops in general and Italian coffee shops particularly. This, of course, does not imply that those zones are actually optimal locations for a new coffee shop! Purpose of this analysis was to only provide info on areas close to Berlin center but not crowded with existing coffee shops (particularly Italian) - it is entirely possible that there is a very good reason for small number of coffee shops in any of those areas, reasons which would make them unsuitable for a new coffee shop regardless of lack of competition in the area. Recommended zones should therefore be considered only as a starting point for more detailed analysis which could eventually result in location which has not only no nearby competition but also other factors taken into account and all other relevant conditions met.

##Conclusion

Purpose of this project was to identify Shanghai areas close to center with low number of coffee shop in order to aid stakeholders in narrowing down the search for optimal location for a new coffee shop. By calculating density distribution from Foursquare data we have first identified general boroughs that justify further analysis, and then generated extensive collection of locations which satisfy some basic requirements regarding existing nearby coffee shop. Clustering of those locations was then performed in order to create major zones of interest (containing greatest number of potential locations) and addresses of those zone centers were created to be used as starting points for final exploration by stakeholders.

Final decission on optimal coffee shop location will be made by stakeholders based on specific characteristics of neighborhoods and locations in every recommended zone, taking into consideration additional factors like attractiveness of each location (proximity to park or water), levels of noise / proximity to major roads, real estate availability, prices, social and economic dynamics of every neighborhood etc.
"""



"""##Thanks..."""

